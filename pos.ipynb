{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import conll2000\n",
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('conll2000', quiet=True)\n",
    "nltk.download('tagsets', quiet=True)\n",
    "tagged_sentences = list(conll2000.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB'), ('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN'), ('if', 'IN'), ('trade', 'NN'), ('figures', 'NNS'), ('for', 'IN'), ('September', 'NNP'), (',', ','), ('due', 'JJ'), ('for', 'IN'), ('release', 'NN'), ('tomorrow', 'NN'), (',', ','), ('fail', 'VB'), ('to', 'TO'), ('show', 'VB'), ('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN'), ('from', 'IN'), ('July', 'NNP'), ('and', 'CC'), ('August', 'NNP'), (\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10948\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "train_set, test_set = train_test_split(tagged_sentences,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set([word for sentence in train_set for word, tag in sentence])\n",
    "tags = set([tag for sentence in train_set for word, tag in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = len(vocabulary)\n",
    "n_states = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
    "tag_to_idx = {tag: i for i, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21076, 44)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_obs, n_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emission Probabilities\n",
    "emission_prob = np.zeros((n_states, n_obs))\n",
    "epsilon = 1e-6\n",
    "for sentence in train_set:\n",
    "    for word, tag in sentence:\n",
    "        emission_prob[tag_to_idx[tag], word_to_idx[word]] += 1\n",
    "emission_prob = (emission_prob + epsilon) / (np.sum(emission_prob, axis=1, keepdims=True) + epsilon * n_obs)# Laplace smoothening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transition Probabilities\n",
    "transition_prob = np.zeros((n_states, n_states))\n",
    "for sentence in train_set:\n",
    "    for i in range(len(sentence) - 1):\n",
    "        tag1, tag2 = sentence[i][1], sentence[i + 1][1]\n",
    "        transition_prob[tag_to_idx[tag1], tag_to_idx[tag2]] += 1\n",
    "most_common_tag = np.argmax(np.sum(transition_prob, axis=1))\n",
    "transition_prob = (transition_prob + epsilon) / (np.sum(transition_prob, axis=1, keepdims=True) + epsilon * n_states) # Laplace smoothening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial State Probabilities\n",
    "initial_state_prob = np.zeros(n_states)\n",
    "for sentence in train_set:\n",
    "    tag = sentence[0][1]\n",
    "    initial_state_prob[tag_to_idx[tag]] += 1\n",
    "initial_state_prob = (initial_state_prob + epsilon) / (np.sum(initial_state_prob) + epsilon * n_states) # Laplace smoothening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unknown words in test set, we are going to use the most common tag for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(sentence):\n",
    "    viterbi = np.zeros((n_states, len(sentence)))\n",
    "    backpointer = np.zeros((n_states, len(sentence)), dtype=int)\n",
    "    for i, word in enumerate(sentence):\n",
    "        if i == 0:\n",
    "            viterbi[:, i] = np.log(emission_prob[:, word_to_idx.get(word, most_common_tag)]) + np.log(initial_state_prob)\n",
    "        else:\n",
    "            viterbi[:, i] = np.log(emission_prob[:, word_to_idx.get(word, most_common_tag)]) + np.max(viterbi[:, i - 1] + np.log(transition_prob.T), axis=1)\n",
    "            backpointer[:, i] = np.argmax(viterbi[:, i - 1] + np.log(transition_prob.T), axis=1)\n",
    "    prediction = [np.argmax(viterbi[:, -1])]\n",
    "    for i in range(len(sentence) - 1, 0, -1):\n",
    "        prediction.append(backpointer[prediction[-1], i])\n",
    "    prediction = [list(tag_to_idx.keys())[list(tag_to_idx.values()).index(i)] for i in prediction[::-1]]\n",
    "    return list(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tets Accuracy: 0.9433873250511087\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for sentence in test_set:\n",
    "    tags = [tag for word, tag in sentence]\n",
    "    sentence = [word for word, tag in sentence]\n",
    "    prediction = viterbi_algorithm(sentence)\n",
    "    for p, tag in zip(prediction, tags):\n",
    "        if p == tag:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "accuracy = correct / total\n",
    "print('Tets Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tags: ['PRP', 'VBP', 'VBG', 'TO', 'VB', 'RB', 'RB', 'VBG']\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = 'You are going to have so much fun'.split()\n",
    "predicted_tags = viterbi_algorithm(test_sentence)\n",
    "print('Predicted Tags:', predicted_tags)\n",
    "[nltk.help.upenn_tagset(pred) for pred in predicted_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
